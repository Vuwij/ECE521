{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherwins\\AppData\\Local\\Continuum\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def loadData(fileName):\n",
    "    with np.load(fileName) as data:\n",
    "        Data, Target = data[\"images\"], data[\"labels\"]\n",
    "        np.random.seed(521)\n",
    "        randIdx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIdx)\n",
    "        Data = Data[randIdx]/255\n",
    "        Target = Target[randIdx]\n",
    "        trainData, trainTarget = Data[:15000], Target[:15000]\n",
    "        validData, validTarget = Data[15000:16000], Target[15000:16000]\n",
    "        testData, testTarget = Data[16000:], Target[16000:]\n",
    "    return trainData, trainTarget, validData, validTarget, testData, testTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size is 15000, Valid Data Size is 1000, Test Data Size is 2724\n",
      "[5 9 9 ... 2 0 9]\n"
     ]
    }
   ],
   "source": [
    "fileName = \"notMNIST.npz\"\n",
    "trainData, trainTarget, validData, validTarget, testData, testTarget = loadData(fileName)\n",
    "\n",
    "trainDataSize = len(trainData)\n",
    "validDataSize = len(validData)\n",
    "testDataSize = len(testData)\n",
    "\n",
    "print(\"Train Data Size is %d, Valid Data Size is %d, Test Data Size is %d\" \n",
    "      %(trainDataSize, validDataSize, testDataSize))\n",
    "\n",
    "print(trainTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Feedforward fully connected neural networks\n",
    "===\n",
    "Implement a simple neural network with one hidden layer and 1000 hidden units. Train your neural network on the entire notMNIST training set of ten classes. Because the neural network loss functions are non-convex, a proper weights initialization scheme is crucial to prevent vanishing gradient during back-propagation as a result of learning stuck at a plateau at the beginning of the training. You will use the Xavier initialization to initialize the weight matrices for all the neural networks in this assignment. That is, each weight matrix is initialized from zero-mean independent Gaussians whose variance is 3/(#input_units + #output_units). Unlike the weight matrices, the bias units will be initalized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 layer-wise building block\n",
    "===\n",
    "Write a vectorized Tensorflow Python function that takes the hidden activations from the preivous layer then return the weighted sum of the inputs (i.e. the z) for the current hidden layer. You will also initailize the weight matrix and the biases in the same function. You should use Xavier initialization for the weight matrix. Your function should be able to compute the weighted sum for all the data points in your mini-batch at once using matrix multiplication. It should not contain loops over the training exmaples in the mini-batch. The function should accept two arguments, the input tensor and the number of the hidden units. Include the snippets of the Python code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layerWiseBuildingBlock(X, numHiddenUnits):\n",
    "    \"\"\"Takes the hidden activations from the previous layer then return the weighted sum\n",
    "    of the inputs for the current hidden layer\"\"\"\n",
    "    # INPUT: input tensor and the number of the hidden units\n",
    "    # Output: the weighted sum of the inputs for the current hidden layer\n",
    "    prevDim = tf.to_int32(X.get_shape()[1])\n",
    "    std_dev = tf.to_float(3/(prevDim + numHiddenUnits))\n",
    "    \n",
    "    # Variable Creation\n",
    "    S = tf.placeholder(tf.float32, [None, numHiddenUnits])\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[prevDim,numHiddenUnits], stddev=std_dev))\n",
    "    b = tf.Variable(0.0)\n",
    "    \n",
    "    # Graph definition\n",
    "    S = tf.matmul(X, W) + b  # dim is [None, numHiddenUnits]\n",
    "    \n",
    "    return S, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    c = tf.constant([[1,2],[3,4]], dtype=tf.float32)\n",
    "    print(sess.run(c))\n",
    "\n",
    "    prevDim = tf.shape(c)[1]\n",
    "    print(sess.run(prevDim))\n",
    "\n",
    "    layerWiseBuildingBlock(c, 1000)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Learning\n",
    "===\n",
    "Use your function from the previous question to build your neural network model with ReLU activation functions in TensorFlow and tf.nn.relu can be useful. For training your network, you are supposed to find a reasonable value for your learning rate. You should train your neural network for different values of learning rate and choose the one that gives you the fastest convergence in terms of the training loss function. (You might want to \"babysit\" your experiments and terminate a particular run prematurely as soon as you find out that the learning rate value is not very good.) Trying 3 different values should be enough. You may also find it useful to apply a small amount of weight decay to prevent overfitting. (e.g. lambda=3e-4). On the training set, validation set and test set, record your classification erros and cross-entropy losses after each epoch. Plot the training, validation, and test classification error vs. the number of epochs. Make a second plot for the cross-entropy loss vs. the number of epochs. Comment on  your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildGraph(numLayers, numHiddenUnits, learningRate):\n",
    "    \"\"\"Build neural network model with ReLU activation functions\"\"\"\n",
    "    \n",
    "    # Variable creation\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28], name='input_x')\n",
    "    X_flatten = tf.reshape(X, [-1, 28*28])\n",
    "    y_target = tf.placeholder(tf.float32, name='target_y')\n",
    "    y_onehot = tf.one_hot(tf.to_int32(y_target), 10, 1.0, 0.0, axis=-1)\n",
    "    Lambda = tf.placeholder(\"float32\", name='Lambda')\n",
    "    \n",
    "    # Graph definition\n",
    "    # Input <=> Hidden\n",
    "    S1, W1 = layerWiseBuildingBlock(X_flatten, numHiddenUnits)\n",
    "    thetaS1 = tf.nn.relu(S1)\n",
    "    \n",
    "    # Hidden <=> Output\n",
    "    S2, W2 = layerWiseBuildingBlock(thetaS1, 10)\n",
    "    #thetaS2 = tf.nn.relu(S2)\n",
    "    \n",
    "    # Final output layer\n",
    "    y_logit = tf.nn.relu(S2)\n",
    "    y_predicted = tf.nn.softmax(y_logit)\n",
    "    \n",
    "    # Error and accuracy definition\n",
    "    crossEntropyError = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                      labels=y_onehot, logits=y_logit),\n",
    "                                      name='mean_cross_entropy')\n",
    "    acc = tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(y_predicted, -1),\n",
    "                                             tf.to_int64(y_target))))\n",
    "    weightLoss = (tf.reduce_sum(W1*W1) + tf.reduce_sum(W2*W2)) * Lambda * 0.5\n",
    "    loss = crossEntropyError + weightLoss\n",
    "    \n",
    "    # Training mechanism\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate)\n",
    "    train = optimizer.minimize(loss=loss)\n",
    "    \n",
    "    return X, y_target, y_predicted, crossEntropyError, train, Lambda, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:   0, CrossEntropyError: 2.30\n",
      "Iter: 100, CrossEntropyError: 0.69\n",
      "Iter: 200, CrossEntropyError: 0.74\n",
      "Iter: 300, CrossEntropyError: 0.73\n",
      "Iter: 400, CrossEntropyError: 0.65\n",
      "Iter: 500, CrossEntropyError: 0.64\n",
      "Iter: 600, CrossEntropyError: 0.66\n",
      "Iter: 700, CrossEntropyError: 0.70\n",
      "Iter: 800, CrossEntropyError: 0.62\n",
      "Iter: 900, CrossEntropyError: 0.66\n",
      "Iter: 1000, CrossEntropyError: 0.65\n",
      "Iter: 1100, CrossEntropyError: 0.68\n",
      "Iter: 1200, CrossEntropyError: 0.65\n",
      "Iter: 1300, CrossEntropyError: 0.68\n",
      "Iter: 1400, CrossEntropyError: 0.71\n",
      "Iter: 1500, CrossEntropyError: 0.60\n",
      "Iter: 1600, CrossEntropyError: 0.67\n",
      "Iter: 1700, CrossEntropyError: 0.64\n",
      "Iter: 1800, CrossEntropyError: 0.62\n",
      "Iter: 1900, CrossEntropyError: 0.61\n",
      "Iter: 2000, CrossEntropyError: 0.71\n",
      "Iter: 2100, CrossEntropyError: 0.65\n",
      "Iter: 2200, CrossEntropyError: 0.70\n",
      "Iter: 2300, CrossEntropyError: 0.70\n",
      "Iter: 2400, CrossEntropyError: 0.64\n",
      "Iter: 2500, CrossEntropyError: 0.70\n",
      "Iter: 2600, CrossEntropyError: 0.66\n",
      "Iter: 2700, CrossEntropyError: 0.63\n",
      "Iter: 2800, CrossEntropyError: 0.64\n",
      "Iter: 2900, CrossEntropyError: 0.65\n",
      "Iter: 3000, CrossEntropyError: 0.61\n",
      "Iter: 3100, CrossEntropyError: 0.66\n",
      "Iter: 3200, CrossEntropyError: 0.67\n",
      "Iter: 3300, CrossEntropyError: 0.72\n",
      "Iter: 3400, CrossEntropyError: 0.68\n",
      "Iter: 3500, CrossEntropyError: 0.62\n",
      "Iter: 3600, CrossEntropyError: 0.68\n",
      "Iter: 3700, CrossEntropyError: 0.69\n",
      "Iter: 3800, CrossEntropyError: 0.67\n",
      "Iter: 3900, CrossEntropyError: 0.70\n",
      "Iter: 4000, CrossEntropyError: 0.67\n",
      "Iter: 4100, CrossEntropyError: 0.71\n",
      "Iter: 4200, CrossEntropyError: 0.74\n",
      "Iter: 4300, CrossEntropyError: 0.64\n",
      "Iter: 4400, CrossEntropyError: 0.66\n",
      "Iter: 4500, CrossEntropyError: 0.70\n",
      "Iter: 4600, CrossEntropyError: 0.63\n",
      "Iter: 4700, CrossEntropyError: 0.64\n",
      "Iter: 4800, CrossEntropyError: 0.61\n",
      "Iter: 4900, CrossEntropyError: 0.66\n",
      "Iter: 5000, CrossEntropyError: 0.65\n",
      "Iter: 5100, CrossEntropyError: 0.66\n",
      "Iter: 5200, CrossEntropyError: 0.66\n",
      "Iter: 5300, CrossEntropyError: 0.61\n",
      "Iter: 5400, CrossEntropyError: 0.65\n",
      "Iter: 5500, CrossEntropyError: 0.69\n",
      "Iter: 5600, CrossEntropyError: 0.68\n",
      "Iter: 5700, CrossEntropyError: 0.64\n",
      "Iter: 5800, CrossEntropyError: 0.61\n",
      "Iter: 5900, CrossEntropyError: 0.66\n",
      "Iter: 6000, CrossEntropyError: 0.69\n",
      "Iter: 6100, CrossEntropyError: 0.63\n",
      "Iter: 6200, CrossEntropyError: 0.62\n",
      "Iter: 6300, CrossEntropyError: 0.65\n",
      "Iter: 6400, CrossEntropyError: 0.61\n",
      "Iter: 6500, CrossEntropyError: 0.67\n",
      "Iter: 6600, CrossEntropyError: 0.63\n",
      "Iter: 6700, CrossEntropyError: 0.71\n",
      "Iter: 6800, CrossEntropyError: 0.67\n",
      "Iter: 6900, CrossEntropyError: 0.69\n",
      "Iter: 7000, CrossEntropyError: 0.62\n",
      "Iter: 7100, CrossEntropyError: 0.61\n",
      "Iter: 7200, CrossEntropyError: 0.65\n",
      "Iter: 7300, CrossEntropyError: 0.67\n",
      "Iter: 7400, CrossEntropyError: 0.69\n",
      "Iter: 7500, CrossEntropyError: 0.67\n",
      "Iter: 7600, CrossEntropyError: 0.64\n"
     ]
    }
   ],
   "source": [
    "# SGD Implementation\n",
    "# Run over 100~200 epoches\n",
    "B = 500\n",
    "max_iter = 50000\n",
    "wd_lambda = 0.0\n",
    "numBatches = np.floor(len(trainData)/B)\n",
    "\n",
    "trainLoss_list = []\n",
    "validLoss_list = []\n",
    "testLoss_list = []\n",
    "\n",
    "trainAcc_list = []\n",
    "validAcc_list = []\n",
    "testAcc_list = []\n",
    "\n",
    "numLayers=1\n",
    "numHiddenUnits = 1000\n",
    "learningRate = 0.001\n",
    "X, y_target, y_predicted, crossEntropyError, train, Lambda, acc = buildGraph(numLayers, numHiddenUnits, learningRate)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(0, max_iter+1):\n",
    "    if step % numBatches == 0:\n",
    "        # Sample minibatch without replacement\n",
    "        randIdx = np.arange(len(trainData))\n",
    "        np.random.shuffle(randIdx)\n",
    "        trainData = trainData[randIdx]\n",
    "        trainTarget = trainTarget[randIdx]\n",
    "        i = 0  # cyclic index for mini-batch\n",
    "        \n",
    "#         # storing MSE and Acc for the three datasets every epoch\n",
    "#         err = meanSquaredError.eval(feed_dict={X: trainData, y_target: trainTarget})\n",
    "#         acc = np.mean((y_predicted.eval(feed_dict={X: trainData}) > 0.5) == trainTarget)\n",
    "#         trainLoss_list.append(err)\n",
    "#         trainAcc_list.append(acc)\n",
    "        \n",
    "#         err = meanSquaredError.eval(feed_dict={X: validData, y_target: validTarget})\n",
    "#         acc = np.mean((y_predicted.eval(feed_dict={X: validdata}) > 0.5) == validTarget)\n",
    "#         validLoss_list.append(err)\n",
    "#         validAcc_list.append(acc)\n",
    "        \n",
    "#         err = meanSquaredError.eval(feed_dict={X: testData, y_target: testTarget})\n",
    "#         acc = np.mean((y_predicted.eval(feed_dict={X: testData}) > 0.5) == testTarget)\n",
    "#         testLoss_list.append(err)\n",
    "#         testAcc_list.append(acc)\n",
    "        \n",
    "    # Slicing a mini-batch from the whole training dataset\n",
    "    feeddict = {X: trainData[i*B:(i+1)*B], y_target: trainTarget[i*B:(i+1)*B],\n",
    "               Lambda: wd_lambda}\n",
    "    \n",
    "    # Update model parameters\n",
    "    _, err, yhat = sess.run([train, crossEntropyError, y_predicted], feed_dict=feeddict)\n",
    "    \n",
    "    # storing weights every iteration\n",
    "    # wList.append(currentW)\n",
    "    i += 1\n",
    "    \n",
    "    # displaying training MSE error every 100 iterations\n",
    "    if not (step % 100):\n",
    "        print(\"Iter: %3d, CrossEntropyError: %4.2f\" % (step, err))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Early stopping\n",
    "===\n",
    "Early stopping is the simplest procedure to avoid overfitting. Determine and highlight the early stopping point on the classification error plot from question 1.1.2, and report the training, validation and test classification error at the early stopping point. Are the early stopping points the same on the two plots? Why or why not? Which plot should be used for early stopping, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
